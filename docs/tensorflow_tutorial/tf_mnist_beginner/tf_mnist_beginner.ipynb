{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST For ML Beginners\n",
    "\n",
    "## The MNIST Data\n",
    "\n",
    "Download the MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into three parts:\n",
    "\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <th>training data</th><td>55,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>test data</th><td>10,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>validation data</th><td>5,000</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This split is very important: it's essential that we have separate data which we <b>don't</b> learn from so that we can make sure that what we've learned actually generalizes!\n",
    "\n",
    "Each image is 28 pixels by 28 pixels, 8-bit grayscale. We can flatten each image into a 784-dimensional vector. For now, we are giving up information about the 2D structure, which will be exploited in later tutorials.\n",
    "\n",
    "The result is that `mnist.train.images` is a teusor with a shape of [55000, 784]. Each entry is a pixel intensity between 0 and 1(normalized).\n",
    "\n",
    "Each image in MNIST has a corresponding label(i.e. the 'correct' answer), a number between 0 and 9 representing the digit drawn in the image. \n",
    "\n",
    "In the program, the labels are converted to 'one-hot vectors, whose entries contain exactly one '1' and all others are '0's. For example, label 3 would be [0,0,0,1,0,0,0,0,0,0]. Consequently, mnist.train.labels is a tensor with shape [55000, 10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regressions\n",
    "\n",
    "There are only ten possible things that a given image in MNIST can be, and we want to be able to look at an image and give the <b>probabilities</b> for it being each digit.\n",
    "\n",
    "Softmax regression is a suitable model for our purpose since it gives us a list of values between 0 and 1 that add up to 1. Even later on, when we train more sophisticated models, the final step will be a layer of softmax.\n",
    "\n",
    "A softmax regression has two steps: first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities.\n",
    "\n",
    "The <b>evidence</b> for a certain class $i$ given an input $x$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> $$\\text{evidence}_i = \\sum_j W_{i,~ j} x_j + b_i$$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $W_i$ is the weights and $b_i$ is the bias for class $i$,\n",
    "and $j$ is an index for summing over the pixels in our input image $x$.\n",
    "We then convert the evidence tallies into our predicted probabilities\n",
    "$y$ using the \"softmax\" function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> $$y = \\text{softmax}(\\text{evidence})$$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here softmax is serving as an \"activation\" or \"link\" function, shaping\n",
    "the output of our linear function into the form we want -- in this case, a\n",
    "probability distribution over 10 cases.\n",
    "We can think of it as converting tallies\n",
    "of evidence into probabilities of our input being in each class.\n",
    "It's defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{softmax}(x) = \\text{normalize}(\\exp(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we expand that equation out, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> $$\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out <a href=\"http://neuralnetworksanddeeplearning.com/chap3.html#softmax\">here</a> to get more intuition about softmax function.\n",
    "\n",
    "We can write $y$ in the compact form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> $$y = \\text{softmax}(Wx + b)$$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Regression\n",
    "\n",
    "Import TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create symbolic variable `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` is a <b>placeholder</b>, a value that we'll input when we ask TensorFlow to run a computation. Our input is a series of MNIST images, each flattened into a 784-dimensional vector. The input has a shape `[None, 784]`, where `None` means that a dimension can be of <b>any length</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our TensorFlow program, the weights and biases are <b>Variables</b>. Their values can be easily adjusted by the program during the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `tf.zeros(shape)` produces a tensor of all-zero with a certain shape.\n",
    "\n",
    "We can now implement our model. It only takes one line to define it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tf.matmul(x, W) + b`: the TensorFlow expression for our model $y = Wx + b$. Note that the order of `x` and `W` has been flipped in TF expression since we must match dimensions in matrix multiplication (`x` has shape `[None, 784]`, `W` has shape `[784, 10]`). `b` is the bias term.\n",
    "* `tf.nn.softmax(...)`: the softmax regression of our 'evidences'. `nn` means 'neural network'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In machine learning, the term <b>loss</b> represents how far off our model's prediction is from the desired outcome. The goal of <b>training</b> is to minimize the loss.\n",
    "\n",
    "A commonly-used loss function is called <b>cross entropy</b>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> $$H_{y'}(y) = -\\sum_i y'_i \\log(y_i)$$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $y$ is our predicted probabiligy distribution and  $y'$ is the true distribution (the one-hot vector with the digit labels). It's good to <a href=\"http://colah.github.io/posts/2015-09-Visual-Information/\">understand</a> how cross entropy works.\n",
    "\n",
    "For implementation, first add a new placeholder for correct answers(labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the cross-entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tf.log`: the logarithm operation.\n",
    "* `y_*tf.log(y)`: element-wise multiplication\n",
    "* `tf.reduce_sum(..., axis=1)`: see example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axis=0: [ 3.  3.]\n",
      "axis=1: [ 2.  4.]\n"
     ]
    }
   ],
   "source": [
    "mat = tf.Variable([[1, 1], \n",
    "                 [2, 2]], dtype=tf.float32)\n",
    "init_op =  tf.global_variables_initializer()\n",
    "\n",
    "sum0_op = tf.reduce_sum(mat, axis=0)\n",
    "sum1_op = tf.reduce_sum(mat, axis=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op) # initialize global variables\n",
    "    sum0_val = sess.run(sum0_op)\n",
    "    sum1_val = sess.run(sum1_op)\n",
    "    print 'axis=0:', sum0_val\n",
    "    print 'axis=1:', sum1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tf.reduce_mean(...)`: similar to `tf.reduce_sum()`\n",
    "\n",
    "For more details, see documentation: <a href=\"https://www.tensorflow.org/api_docs/python/tf/reduce_mean\">tf.reduce_mean</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/reduce_sum\">tf.reduce_sum</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the source code, we don't use this formulation since it's numerically unstable (previously  in `tf.nn.softmax(arg)`, since `arg` is unnormalized, the entries can be very large, so that `exp(arg)` might be ridiculously large). Instead, we apply `tf.nn.softmax_cross_entropy_with_logits` on the unnormalized logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(x, W) + b # use the unnormalized model (no softmax)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info, see <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\">tf.nn.softmax_cross_entropy_with_logits</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we want our model to do, it's very easy to have TensorFlow train it to do so. Because TensorFlow knows the entire graph of your computations, it can automatically use the <a href=\"http://colah.github.io/posts/2015-08-Backprop/\"><b>backpropagation algorithm</b></a> to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we ask TensorFlow to minimize `cross_entropy` using the <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\"><b>gradient descent algorithm</b></a> with a learning rate of 0.5. Gradient descent is a simple procedure, where TensorFlow simply shifts each variable a little bit in the direction that reduces the cost. But TensorFlow also provides <a href=\"https://www.tensorflow.org/api_guides/python/train#Optimizers\">many other optimization algorithms</a>: using one is as simple as tweaking one line.\n",
    "\n",
    "What TensorFlow actually does here, behind the scenes, is to add new operations to your graph which implements backpropagation and gradient descent. Then it gives you back a single operation which, when run, does <b>a step</b> of gradient descent training, slightly tweaking your variables to reduce the loss.\n",
    "\n",
    "We can now launch the model in an `InteractiveSession`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to create an operation to initialize the variables we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train -- we'll run the training step 1000 times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in xrange(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_:batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step of the loop, we get a 'batch' of 100 random data points from our training set. We run `train_step` feeding in the batches data to replace the `placeholder`s.\n",
    "\n",
    "Using small batches of random data is called <b>stochastic training</b> -- in this case, stochastic gradient descent. Ideally, we'd like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that's expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Model\n",
    "\n",
    "How well does our model do?\n",
    "\n",
    "Well, first let's figure out where we predicted the correct label. `tf.argmax` is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. \n",
    "\n",
    "Examples of `tf.argmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0]\n"
     ]
    }
   ],
   "source": [
    "vec = [[1, 2, 3],\n",
    "       [6, 5, 4]]\n",
    "print sess.run(tf.argmax(vec, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, `tf.argmax(y, axis=1)` is the label our model thinks is most likely for each input, while `tf.argmax(y_, axis=1)` is the correct label.\n",
    "\n",
    "We can use tf.equal to check if our prediction matches the truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us a list of booleans. To determine what fraction are correct, we cast (type conversion) to floating point numbers ans then take the mean. For example, `[True, False, True, True]` would become `[1, 0, 1, 1]` which would become `0.75`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we ask for our accuracy on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8742\n"
     ]
    }
   ],
   "source": [
    "print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is not very good since our model is too young too simple. To get better results, we need a more sophisticated model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
